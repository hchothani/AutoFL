# PLoRA on CIFAR10 - Parameter-Efficient Low-Rank Adaptation for FL
# Based on: LoRA and Parameter-Efficient Fine-Tuning for Federated Learning

dataset:
  workload: cifar10
  batch_size: 32
  num_classes: 10

model:
  name: simple_cnn
  num_classes: 10

cl:
  strategy: plora
  num_experiences: 5
  split: random

# PLoRA-specific configurations
plora:
  rank: 4  # LoRA rank
  alpha: 1.0  # LoRA alpha scaling factor

training:
  learning_rate: 0.001
  epochs: 3
  optimizer: adam

server:
  strategy: fedavg  # PLoRA uses standard FedAvg aggregation
  num_rounds: 10
  num_clients: 3
  fraction_fit: 1.0
  fraction_eval: 1.0
  min_fit: 3
  min_eval: 3

client:
  num_cpus: 4
  num_gpus: 0.0  # CPU for testing
  epochs: 3
  falloff: 0.0
  exp_epochs: null  # Use server num_rounds instead

wb:
  project: autofl-sota-testing
  name: plora_cifar10_simplecnn 