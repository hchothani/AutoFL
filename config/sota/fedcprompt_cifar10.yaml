# FedCPrompt on CIFAR10 - Federated Continual Learning with Prompts
# Based on: Federated Class-Incremental Learning via Prompting

dataset:
  workload: cifar10
  batch_size: 32
  num_classes: 10

model:
  name: simple_cnn
  num_classes: 10

cl:
  strategy: fedcprompt
  num_experiences: 5
  split: random

# FedCPrompt-specific configurations
fedcprompt:
  prompt_length: 8  # Length of prompt tokens
  prompt_lr: 0.01  # Learning rate for prompt optimization

training:
  learning_rate: 0.001
  epochs: 3
  optimizer: adam

server:
  strategy: fedavg  # FedCPrompt uses standard FedAvg aggregation
  num_rounds: 10
  num_clients: 3
  fraction_fit: 1.0
  fraction_eval: 1.0
  min_fit: 3
  min_eval: 3

client:
  num_cpus: 4
  num_gpus: 0.0  # CPU for testing
  epochs: 3
  falloff: 0.0
  exp_epochs: null  # Use server num_rounds instead

wb:
  project: autofl-sota-testing
  name: fedcprompt_cifar10_simplecnn 